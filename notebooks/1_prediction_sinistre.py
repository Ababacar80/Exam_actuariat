# -*- coding: utf-8 -*-
"""1.prediction-sinistre.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qbmJ-y43qMHwAuZoGlhOqoPfu0TS7FV7

# PrÃ©dire les sinistres (demande d'indemnisation)

*(Si vous nâ€™avez pas ces packages, installezâ€‘les avec `pip install pandas seaborn matplotlib`.)*
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Affichage plus lisible
pd.set_option('display.max_columns', None)
sns.set_style('whitegrid')

"""## 2. Chargement du jeu de donnÃ©es
Nous utilisons un fichier CSV contenantÂ :

| Colonne | Signification | Exemple |
|---------|---------------|---------|
| `age` | Ã‚ge de lâ€™assurÃ©Â·e | 29 |
| `sex` | Genre (male / female) | female |
| `bmi` | Indice de Masse Corporelle | 27.9 |
| `bloodpressure` | Tension artÃ©rielle moyenne | 125 |
| `children` | Nombre dâ€™enfants Ã  charge | 2 |
| `smoker` | Statut fumeur (yes / no) | yes |
| `region` | Zone gÃ©ographique | southwest |
| `charges` | Montant du sinistre (â‚¬) | 16â€¯854 |

Ces variables sont couramment utilisÃ©es par les actuaires pour segmenter le risque et ajuster la prime dâ€™assurance.
"""

# Modifiez le chemin si nÃ©cessaire
file_path = 'datasets/insurance-demographic-health.csv'

data = pd.read_csv(file_path)
print(f"Dimensions : {data.shape}")
data.head()

# Modifiez le chemin si nÃ©cessaire
file_path = '/content/sample_data/insurance-demographic-health.csv'

data = pd.read_csv(file_path)
print(f"Dimensions : {data.shape}")
data.head()

df = data.copy()

df.info()

"""## 3. Nettoyage des donnÃ©es
Dans un contexte dâ€™assurance, **la qualitÃ© des donnÃ©es est critique**Â :
1. Les valeurs manquantes peuvent biaiser les calculs de frÃ©quence et de coÃ»t moyen.
2. Les doublons faussent les statistiques (par ex. un assurÃ© comptÃ© deux fois).

Nous allons :
* dÃ©tecter les lignes concernÃ©es ;
* les supprimer pour simplifier (dâ€™autres optionsâ€¯: imputation, correction manuelle).

> En production, un pipeline de Dataâ€¯Engineering/ETL gÃ¨re gÃ©nÃ©ralement ces Ã©tapes automatiquement.
"""

df.isnull().any()

sns.heatmap(df.isnull())

(df.isna().sum() / len(df)) * 100

"""- Les valeurs manquantes representent moins de `1%` du jeu de donnÃ©es"""

# --- Valeurs manquantes ---
missing_rows = df[df.isnull().any(axis=1)]
print(f"Lignes avec au moins une valeur manquante : {len(missing_rows)}")

# --- Doublons ---
duplicate_rows = df[df.duplicated()]
print(f"Lignes dupliquÃ©es : {len(duplicate_rows)}")

# Suppression (conservative)
df = df.dropna().drop_duplicates()
print("Dimensions aprÃ¨s nettoyage :", df.shape)

"""> **Pourquoi ne pas toujours imputerâ€¯?**
>
> * Dans lâ€™assurance santÃ©, remplacer une tension manquante par la moyenne peut masquer un risque rÃ©el.
> * Pour un exercice pÃ©dagogique, il est plus simple de supprimer â€“ mais retenez quâ€™en production on privilÃ©gie lâ€™imputation supervisÃ©e ou la collecte de donnÃ©es manquantes auprÃ¨s de lâ€™assurÃ©.

## 4. Ã‚ge, genre et montant des sinistres
Les assureurs vÃ©rifient souvent si le **coÃ»t moyen** Ã©volue avec lâ€™Ã¢ge et diffÃ¨re selon le genre.
"""

df.columns

df['bmi'].max()

numerical=df.select_dtypes(exclude='object')

sns.heatmap(numerical.corr(), annot=True)

# CorrÃ©lation linÃ©aire Ã¢ge â†” charges
age_claim_corr = df['age'].corr(df['claim'])
print(f"CorrÃ©lation Ã¢ge / charges : {age_claim_corr:.3f}")

# Statistiques par genre
stats_genre = df.groupby('gender')['claim'].agg(mean='mean', median='median', count='size')
stats_genre

"""### Visualisation"""

plt.figure(figsize=(8,6))
sns.scatterplot(x='age', y='claim', hue='gender', data=df, alpha=0.6)
plt.title('Ã‚ge vs Montant des sinistres (couleur = genre)')
plt.show()

plt.figure(figsize=(6,5))
sns.barplot(x=stats_genre.index, y='mean', data=stats_genre.reset_index(), palette='viridis')
plt.title('Montant moyen des sinistres par genre')
plt.ylabel('â‚¬')
plt.show()

"""**InterprÃ©tation :**
* Un coefficient proche de 0 signifie peu ou pas de relation **linÃ©aire**. Dâ€™autres formes (nonâ€‘linÃ©aires) peuvent exister.
* Si les hommes ont un coÃ»t moyen plus Ã©levÃ©, cela peut venir de comportements de risque (tabagisme, IMC, etc.).

## 5. IMC, tension artÃ©rielle et sinistres
Lâ€™IMC et la tension sont deux indicateurs de santÃ© clÃ©s.
* **IMC Ã©levÃ©**â€¯: risque dâ€™obÃ©sitÃ© â†’ maladies cardiovasculaires â‡’ coÃ»ts Ã©levÃ©s.
* **Hypertension**â€¯: facteur de risque majeur pour AVC / infarctus.
"""

corr_age = df['age'].corr(df['bloodpressure'])
corr_bmi = df['bmi'].corr(df['claim'])
corr_bp  = df['bloodpressure'].corr(df['claim'])
print(f"CorrÃ©lation IMC / charges : {corr_bmi *100:.2f} %")
print(f"CorrÃ©lation tension / charges : {corr_bp*100:.2f} %")
print(f"CorrÃ©lation age / tension arterielle : {corr_age*100:.2f} %")

df.columns

plt.figure(figsize=(8,6))
sns.scatterplot(x='PatientID', y='claim', hue='gender', data=df, alpha=0.6)
plt.title('Ã‚ge vs Montant des sinistres (couleur = genre)')
plt.show()

plt.figure(figsize=(6,5))
sns.barplot(x=stats_genre.index, y='mean', data=stats_genre.reset_index(), palette='viridis')
plt.title('PatienID vs Montant Sinistre')
plt.ylabel('â‚¬')
plt.show()

df.tail(10)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.scatterplot(data=data, x='bmi', y='claim', alpha=0.5, color='steelblue')
plt.title('IMC vs Charges')
plt.subplot(1,2,2)
sns.scatterplot(data=data, x='bloodpressure', y='claim', alpha=0.5, color='seagreen')
plt.title('Tension vs Charges')
plt.tight_layout()
plt.show()

"""On observe gÃ©nÃ©ralement une relation **positive**â€¯: plus lâ€™IMC ou la tension augmentent, plus le coÃ»t des sinistres tend Ã  croÃ®tre. Ces informations sont utilisÃ©es dans les **questionnaires de santÃ©** pour ajuster la prime."""

# Statistiques par genre
stats_genre = df.groupby('gender')['bloodpressure'].agg(mean='mean', median='median', count='size')
stats_genre

"""## 6. Impact du tabagisme
Le tabagisme est souvent la variable la plus discriminante pour les assureurs santÃ©.
"""

data.columns

data.sample(5)

mean_smoker = df.loc[df['smoker']=='Yes','claim'].mean()
mean_nonsmoker = df.loc[df['smoker']=='No','claim'].mean()
print(f"CoÃ»t moyen fumeurs    : {mean_smoker:,.2f} â‚¬")
print(f"CoÃ»t moyen nonâ€‘fumeurs : {mean_nonsmoker:,.2f} â‚¬")

df['smoker_bin'] = df['smoker'].map({'No':0,'Yes':1})
corr_smoker = df['smoker_bin'].corr(df['claim'])
print(f"CorrÃ©lation (binaire) tabagisme / charges : {corr_smoker:.3f}")

plt.figure(figsize=(5,4))
plt.bar(['Nonâ€‘fumeur','Fumeur'], [mean_nonsmoker, mean_smoker], color=['skyblue','salmon'])
plt.title('CoÃ»t moyen selon le tabagisme')
plt.ylabel('â‚¬')
plt.show()

"""La corrÃ©lation Ã©levÃ©e (>â€¯0.7) indique que **fumer augmente fortement le coÃ»t des soins**.

> Dans certains pays, les assureurs appliquent une surprime ou imposent un dÃ©lai de carence aux fumeurs.

## 7. Importance des variables avec RandomÂ Forest
Pour prÃ©dire le coÃ»t (`charges`), nous souhaitons connaÃ®tre les variables les plus utiles. La **ForÃªt AlÃ©atoire** fournit un score dâ€™importance basÃ© sur la rÃ©duction dâ€™impuretÃ©.
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor

# Encodage des colonnes catÃ©gorielles
for col in df.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

X = df.drop(columns=['claim'])
y = df['claim']

rf = RandomForestRegressor(n_estimators=300, random_state=42)
rf.fit(X, y)

importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
importances

plt.figure(figsize=(7,4))
importances.head(10).plot(kind='barh')
plt.gca().invert_yaxis()
plt.title('Top 10 des variables les plus importantes')
plt.xlabel('Importance (0-1)')
plt.show()

"""**Comment utiliser cette informationâ€¯?**
* Donner plus de poids aux variables clÃ©s dans la tarification.
* RÃ©duire la dimensionnalitÃ© (simplifier le questionnaire client).
* Communiquer sur les facteurs de risque majeurs auprÃ¨s des assurÃ©s.
"""

from google.colab import drive
drive.mount('/content/drive')

"""# ModelisationÂ :

AprÃ¨s avoir Ã©tudiÃ© lâ€™impact de chaque variable et extrait les facteurs clÃ©s,
passons maintenant Ã  la **prÃ©diction avancÃ©e** avec deux algorithmes de boosting dâ€™arbresÂ :

* **XGBoost** â€“ souvent champion des compÃ©titions Kaggle, robuste aux interactions non linÃ©aires.
* **LightGBM** â€“ implÃ©mentation Microsoft, trÃ¨s rapide et efficace sur gros volumes.

Nous rÃ©utiliserons le jeu de donnÃ©es nettoyÃ© et encodÃ©, puis comparerons leurs performances (MAE, RÂ²).

## Encodage des variables catÃ©gorielles
"""

for col in df.select_dtypes(include='object').columns:
    df[col] = LabelEncoder().fit_transform(df[col])

"""## Split train / test"""

from sklearn.model_selection import train_test_split

X = df.drop(columns=['claim'])
y = df['claim']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## XGBoost

## ModÃ¨le XGBoost (Extreme Gradient Boosting)

### 1. Principe gÃ©nÃ©ral

| ðŸ”                 | Description                                                                                                                                        |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Boosting**       | EntraÃ®ne une succession dâ€™arbres de dÃ©cision **faibles** ; chaque nouvel arbre corrige les erreurs cumulÃ©es des prÃ©cÃ©dents.                        |
| **Gradient**       | Lâ€™erreur rÃ©siduelle est minimisÃ©e : on calcule le **gradient** (dÃ©rivÃ©e) de la fonction perte pour savoir dans quelle direction ajuster le modÃ¨le. |
| **Regularisation** | Ajoute un terme de pÃ©nalitÃ© (L1/L2) sur la complexitÃ© de lâ€™arbre pour Ã©viter lâ€™overfitting.                                                        |
| **ParallÃ©lisme**   | Construction des arbres optimisÃ©e (histogram-based + multi-thread) â†’ entraÃ®nement rapide mÃªme sur gros jeux de donnÃ©es.                            |

### 2. Pourquoi XGBoost est performant sur les sinistres ?

* **Relations non linÃ©aires** : le coÃ»t dâ€™un sinistre santÃ© nâ€™est pas strictement linÃ©aire (effet multiplicatif entre Ã¢ge, IMC, tabagismeâ€¦). Les arbres gÃ¨rent ces interactions naturellement.
* **Robuste aux valeurs aberrantes** : quelques cas de sinistres trÃ¨s chers (queue heavy-tailed) nâ€™influencent pas outre mesure la prÃ©diction globale.
* **Gestion du dÃ©sÃ©quilibre** : paramÃ¨tre `scale_pos_weight` (en classification) ou pondÃ©rations par observation pour sur-reprÃ©senter les sinistres rares mais coÃ»teux.
* **Importance des variables** : fournit des scores Â« gain Â» ou Â« cover Â» utiles pour expliquer la tarification.

### 3. Avantages

| âœ…                             | DÃ©tails                                                                 |
| ----------------------------- | ----------------------------------------------------------------------- |
| **Excellente prÃ©cision**      | Souvent vainqueur de compÃ©titions Kaggle et benchmarks industriels.     |
| **ContrÃ´le overfitting**      | ParamÃ¨tres `gamma`, `lambda`, `alpha`, `subsample`â€¦                     |
| **Feature importance**        | Facile Ã  extraire et Ã  visualiser (gain, cover, weight).                |
| **TolÃ¨re donnÃ©es manquantes** | Choisit automatiquement la meilleure direction de branchement pour NaN. |

### 4. InconvÃ©nients / limites

| âš ï¸                           | Impact                                                                                                             |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **HyperparamÃ¨tres nombreux** | GridSearch long ; nÃ©cessite une bonne maÃ®trise pour Ã©quilibrer biais/variance.                                     |
| **Taille mÃ©moire**           | ModÃ¨les volumineux avec beaucoup dâ€™arbres â†’ empreinte RAM plus Ã©levÃ©e quâ€™un modÃ¨le linÃ©aire.                       |
| **ExplicabilitÃ© partielle**  | Importance globale OK, mais explication locale moins intuitive quâ€™une rÃ©gression logistique (nÃ©cessite SHAP/LIME). |
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score

from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=4,
                   subsample=0.8, colsample_bytree=0.8, random_state=42)
xgb.fit(X_train, y_train)
pred_xgb = xgb.predict(X_test)
print('MAE XGB:', mean_absolute_error(y_test, pred_xgb))
print('RÂ² XGB :', r2_score(y_test, pred_xgb))

"""## LightGBM

## ModÃ¨le LightGBM (Light Gradient Boosting Machine)

### 1. Principe gÃ©nÃ©ral

| ðŸ”                     | Description                                                                                                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Leaf-Wise Growth**   | Contrairement Ã  XGBoost (level-wise), LightGBM grandit dâ€™abord **la branche la plus prometteuse** (leaf-wise) â†’ perte rÃ©duite plus vite.                            |
| **Histogram-Based**    | Les valeurs continues sont discrÃ©tisÃ©es en bacs ; accÃ©lÃ¨re le tri et rÃ©duit la mÃ©moire.                                                                             |
| **GOSS & EFB**         | *Gradient-Based One-Side Sampling* sÃ©lectionne les instants ayant les gradients les plus grands ; *Exclusive Feature Bundling* compacte les features peu corrÃ©lÃ©es. |
| **CatÃ©gories natives** | Accepte des entiers reprÃ©sentant les modalitÃ©s sans one-hot, en appliquant un regroupement optimal.                                                                 |

### 2. Pourquoi LightGBM brille pour le pricing sinistre ?

* **RapiditÃ©** : permet des itÃ©rations rapides sur de gros portefeuilles (millions de contrats).
* **ScalabilitÃ© verticale** : moins gourmand en mÃ©moire â†’ entraÃ®ne un modÃ¨le complet sur un laptop Ã©tudiant.
* **Gestion des interactions** : comme XGBoost, capture efficacement les effets combinatoires (ex. rÃ©gion Ã— catÃ©gorie socio-pro) impactant le coÃ»t.
* **Support des variables catÃ©gorielles** : Ã©vite le *curse of dimensionality* dÃ» au one-hot (ex. 50 rÃ©gions â†’ 50 colonnes).

### 3. Avantages

| âœ…                                      | DÃ©tails                                                               |
| -------------------------------------- | --------------------------------------------------------------------- |
| **EntraÃ®nement ultra-rapide**          | 5-10Ã— plus vite quâ€™un Gradient Boosting classique.                    |
| **Moins de mÃ©moire**                   | Histogram & EFB rÃ©duisent lâ€™usage RAM.                                |
| **Performances comparables Ã  XGBoost** | Souvent Ã©quivalentes, parfois supÃ©rieures sur grands jeux de donnÃ©es. |
| **ParamÃ¨tres intuitifs**               | `num_leaves`, `max_depth`, `learning_rate` â€” tuning plus lÃ©ger.       |

### 4. InconvÃ©nients / limites

| âš ï¸                          | Impact                                                                              |
| --------------------------- | ----------------------------------------------------------------------------------- |
| **Leaf-wise â†’ Overfitting** | Peut sur-apprendre si `num_leaves` est trop grand ou `min_data_in_leaf` trop petit. |
| **Support GPU partiel**     | Accelerations GPU disponibles mais moins matures que XGBoost.                       |
| **InterprÃ©tation locale**   | MÃªme besoin dâ€™outils SHAP pour expliquer chaque prÃ©diction.                         |

---

## Choisir entre XGBoost et LightGBM

| CritÃ¨re                    | XGBoost                   | LightGBM                                 |
| -------------------------- | ------------------------- | ---------------------------------------- |
| **Vitesse entraÃ®nement**   | Rapide mais plus lent     | âš¡ TrÃ¨s rapide                            |
| **Tuning hyperparamÃ¨tres** | Plus nombreux             | Moins nombreux                           |
| **Robustesse NaN**         | Automatique               | Automatique                              |
| **CatÃ©gories natives**     | Non (nÃ©cessite encodage)  | Oui                                      |
| **MÃ©moire**                | Plus Ã©levÃ©e               | Plus faible                              |
| **ExplicabilitÃ©**          | Importance globale & SHAP | Importance globale & SHAP                |
| **PrÃ©cision**              | Excellent                 | Ã‰quivalent ou meilleur sur gros datasets |

---


> **En pratique :**
>
> * Sur un **jeu de donnÃ©es modeste** (â‰¤ 100 k lignes), les deux donnent souvent des rÃ©sultats similaires ; XGBoost peut Ãªtre plus simple Ã  expliquer grÃ¢ce Ã  sa large documentation.
> * Sur un **portefeuille massif** (â‰¥ 1 M de polices) ou avec un temps de calcul limitÃ©, **LightGBM** est souvent prÃ©fÃ©rÃ©.
> * Le choix final doit considÃ©rer : dÃ©lai dâ€™entraÃ®nement, contraintes mÃ©moire, besoin de support GPU, facilitÃ© dâ€™intÃ©gration et niveau dâ€™interprÃ©tabilitÃ© requis par la conformitÃ©.
"""

from lightgbm import LGBMRegressor
lgbm = LGBMRegressor(n_estimators=400, learning_rate=0.05, random_state=42)
lgbm.fit(X_train, y_train)
pred_lgbm = lgbm.predict(X_test)
print('MAE LGBM:', mean_absolute_error(y_test, pred_lgbm))
print('RÂ² LGBM :', r2_score(y_test, pred_lgbm))

"""### Comparaison XGBoost et LightGBM"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import time, numpy as np

def evaluate(model, X_train, X_test, y_train, y_test, name="model"):
    t0 = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - t0

    preds = model.predict(X_test)
    mae  = mean_absolute_error(y_test, preds)
    rmse = mean_squared_error(y_test, preds, squared=False)
    r2   = r2_score(y_test, preds)

    return {"ModÃ¨le": name,
            "MAE (â‚¬)": mae,
            "RMSE (â‚¬)": rmse,
            "RÂ²": r2,
            "Train_time (s)": train_time}

results = []
results.append(evaluate(xgb,  X_train, X_test, y_train, y_test, "XGBoost"))
results.append(evaluate(lgbm, X_train, X_test, y_train, y_test, "LightGBM"))

pd.DataFrame(results).set_index("ModÃ¨le").style.format({"MAE (â‚¬)": "{:,.0f}",
                                                        "RMSE (â‚¬)": "{:,.0f}",
                                                        "Train_time (s)": "{:.1f}"})

"""- ### Validation croisÃ©e"""

from sklearn.model_selection import KFold, cross_val_score

cv = KFold(n_splits=5, shuffle=True, random_state=42)

mae_cv_xgb  = -cross_val_score(xgb,  X, y, cv=cv, scoring="neg_mean_absolute_error")
mae_cv_lgbm = -cross_val_score(lgbm, X, y, cv=cv, scoring="neg_mean_absolute_error")

print("XGB  MAE CV  :", mae_cv_xgb.mean(), "Â±", mae_cv_xgb.std())
print("LGBM MAE CV :", mae_cv_lgbm.mean(), "Â±", mae_cv_lgbm.std())

"""- ### VÃ©rifier lâ€™importance des variables & lâ€™explicabilitÃ©"""

!pip install shap -q

import shap
explainer = shap.TreeExplainer(xgb)   # ou lgbm
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)   # importance + direction

"""- ### Alternative rapide (sans shap)"""

importances = pd.Series(xgb.feature_importances_, index=X.columns).sort_values(ascending=False)
importances.plot(kind='barh', figsize=(6,4))
plt.title("XGBoost â€“ importance des variables (gain)")
plt.show()

importances_lgbm = pd.Series(lgbm.feature_importances_, index=X.columns).sort_values()
importances_lgbm.plot(kind='barh', figsize=(6,4))
plt.title("LightGBM â€“ importance des variables")
plt.show()

